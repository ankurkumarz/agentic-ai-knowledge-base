
## LLM Evaluation Dashboards

- [LMSYS Chatbot Arena](https://lmarena.ai?leaderboard): Community-driven Evaluation for Best LLM and AI chatbots
- [Vellum LLM Comparison Board](https://www.vellum.ai/llm-leaderboard)
- [The Berkeley Function/Tool Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html): evaluates the LLM's ability to call functions (aka tools) accurately. This leaderboard consists of real-world data and will be updated periodically.
- [Galileo Agent Leaderboard](https://huggingface.co/spaces/galileo-ai/agent-leaderboard): LLM Performance in Agentic scenarios. [Version 2](https://huggingface.co/spaces/galileo-ai/agent-leaderboard) is also available.
  
## Evaluation Benchmarks

- [VisualWebArena](https://github.com/web-arena-x/visualwebarena): for multimodal agents
- [GAIA: HF Benchmarking General AI Agents](https://huggingface.co/gaia-benchmark)
- [OSWorld Benchmark for Multimodal Agents](https://os-world.github.io/): Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments
- [SWE-Bench](https://www.swebench.com/): dataset that tests systems' ability to solve GitHub issues automatically.


## LLM Inference Benchmarks Research

- [Inference Benchmarking of Large Language Models on AI Accelerators](https://arxiv.org/html/2411.00136v1): LLM-Inference-Bench, a comprehensive benchmarking suite that evaluates the inference performance of the variety of llama-style LLMs across SOTA AI accelerators
![Time to First Token TTFT](https://arxiv.org/html/2411.00136v1/x26.png)


## Solutions Benchmark

- [vLLM Benchmark for hosting inference LLM models](https://hud.pytorch.org/benchmark/llms): 
