
## LLM Evaluation Dashboards

- [LMSYS Chatbot Arena](https://lmarena.ai?leaderboard): Community-driven Evaluation for Best LLM and AI chatbots
- [Vellum LLM Comparison Board](https://www.vellum.ai/llm-leaderboard)
- [The Berkeley Function/Tool Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html): evaluates the LLM's ability to call functions (aka tools) accurately. This leaderboard consists of real-world data and will be updated periodically.
- [Galileo Agent Leaderboard](https://huggingface.co/spaces/galileo-ai/agent-leaderboard): LLM Performance in Agentic scenarios
  
## Evaluation Benchmarks

- [VisualWebArena](https://github.com/web-arena-x/visualwebarena): for multimodal agents
- [GAIA: HF Benchmarking General AI Agents](https://huggingface.co/gaia-benchmark)
- [OSWorld Benchmark for Multimodal Agents](https://os-world.github.io/): Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments
- [SWE-Bench](https://www.swebench.com/): dataset that tests systems' ability to solve GitHub issues automatically.

