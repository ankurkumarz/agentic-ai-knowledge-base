## Evaluation Benchmarks

- [Galileo Agent Leaderboard](https://huggingface.co/spaces/galileo-ai/agent-leaderboard): LLM Performance in Agentic scenarios 
- [VisualWebArena](https://github.com/web-arena-x/visualwebarena): for multimodal agents
- [GAIA: HF Benchmarking General AI Agents](https://huggingface.co/gaia-benchmark)
- [OSWorld Benchmark for Multimodal Agents](https://os-world.github.io/): Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments
- [SWE-Bench](https://www.swebench.com/): dataset that tests systems' ability to solve GitHub issues automatically. 