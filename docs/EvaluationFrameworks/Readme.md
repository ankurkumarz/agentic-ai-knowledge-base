# Evaluation Frameworks

## LLM Evaluation Frameworks

- [Confident AI DeepEval](https://docs.confident-ai.com/): the open-source LLM eval f/w with 14+ LLM evaluation metrics (both for RAG and fine-tuning use cases)
- [MLFlow LLM Evaluate](https://mlflow.org/docs/latest/llms/llm-evaluate/): a modular and simplistic package that allows you to run evaluations in your own evaluation pipelines. It offers RAG evaluation and QA evaluation.
- [RAGAS](https://www.ragas.io/): Metrics for evaluating RAG(8+ metrics), Agentic workflows (3 metrics) and more.
- [LangChain OpenEvals](https://github.com/langchain-ai/openevals): Based on LLM-as-a-judge with pre-built prompts for conciseness, fairness, hallucination 

## AI Agents Evaluation

- [Meta - MLGym: A Framework & Benchmark for Advancing AI Research Agents](https://arxiv.org/abs/2502.14499), [GitHub](https://github.com/facebookresearch/MLGym)