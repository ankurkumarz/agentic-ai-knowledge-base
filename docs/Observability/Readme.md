
## Guidelines/ Best Practices

- [Observability Guide](https://www.confident-ai.com/blog/what-is-llm-observability-the-ultimate-llm-monitoring-guide)
- [Portkey Guide to LLM observability for 2026](https://portkey.ai/blog/the-complete-guide-to-llm-observability/)

![End-to-end Agent Observability](https://i.postimg.cc/8CzhFKZx/IMG_2615.png)
![Observability Metrics](https://i.postimg.cc/j23VbkkT/IMG-2619.jpg)

## Solutions

- [AgentOps](https://www.agentops.ai): Dev platform + observability
- [Comet Opik](https://www.comet.com/docs/opik/)
- [Langfuse](https://langfuse.com/): (YC W23) Traces, evals, prompt management and metrics to debug and observe
- [LangSmith](https://www.langchain.com/langsmith): dev platform inc. observability
- [Openlit](https://openlit.io/): Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground
- [Weights & Biases (W&B) Weave](https://weave-docs.wandb.ai/): a framework for tracking, experimenting with, evaluating, deploying, and improving LLM-based apps
- [Braintrust](https://www.braintrust.dev/): AI observability platform helping teams measure, evaluate, and improve AI in production. With Braintrust, teams can compare models, iterate on prompts, catch regressions, and leverage real user data to continuously improve AI applications.